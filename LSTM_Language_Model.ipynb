{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Language_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEzPwwNaQ-SL",
        "outputId": "446cbb79-4833-4217-a1cf-d379aee041b9"
      },
      "source": [
        "from google.colab import drive\n",
        "import sys, os\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpijFt6VUNuN",
        "outputId": "442defe1-9231-41f5-c7db-72c843a448c7"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCLHUBW6RaqH"
      },
      "source": [
        "training_data = \"/content/drive/My Drive/Sussex AI/Spring Semester/Advanced NLP/Week 2/sentence-completion/Holmes_Training_Data\""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm-PeJlsRdqY",
        "outputId": "33eab6c7-c8fe-4cdc-a09b-1c2458fcd2fe"
      },
      "source": [
        "text = ''\n",
        "n_docs = 50\n",
        "i = 0\n",
        "for f,file in enumerate(os.listdir(training_data)):\n",
        "  print('Processing file {}'.format(f))\n",
        "  if i >= n_docs:\n",
        "    break\n",
        "  try:\n",
        "    with open(os.path.join(training_data,file)) as instream:\n",
        "      line = instream.read().replace('\\n', '')\n",
        "      text += line\n",
        "      i += 1\n",
        "  except UnicodeDecodeError:\n",
        "    print('Error processing file {}'.format(f))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing file 0\n",
            "Processing file 1\n",
            "Processing file 2\n",
            "Processing file 3\n",
            "Processing file 4\n",
            "Processing file 5\n",
            "Processing file 6\n",
            "Processing file 7\n",
            "Processing file 8\n",
            "Processing file 9\n",
            "Processing file 10\n",
            "Processing file 11\n",
            "Processing file 12\n",
            "Processing file 13\n",
            "Processing file 14\n",
            "Processing file 15\n",
            "Processing file 16\n",
            "Processing file 17\n",
            "Processing file 18\n",
            "Processing file 19\n",
            "Processing file 20\n",
            "Processing file 21\n",
            "Processing file 22\n",
            "Error processing file 22\n",
            "Processing file 23\n",
            "Processing file 24\n",
            "Processing file 25\n",
            "Processing file 26\n",
            "Processing file 27\n",
            "Processing file 28\n",
            "Processing file 29\n",
            "Processing file 30\n",
            "Processing file 31\n",
            "Processing file 32\n",
            "Processing file 33\n",
            "Processing file 34\n",
            "Processing file 35\n",
            "Processing file 36\n",
            "Processing file 37\n",
            "Processing file 38\n",
            "Processing file 39\n",
            "Processing file 40\n",
            "Processing file 41\n",
            "Processing file 42\n",
            "Processing file 43\n",
            "Processing file 44\n",
            "Processing file 45\n",
            "Processing file 46\n",
            "Processing file 47\n",
            "Processing file 48\n",
            "Processing file 49\n",
            "Processing file 50\n",
            "Processing file 51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycGFY36cZoWR"
      },
      "source": [
        "tokenized_text = [w.lower() for w in word_tokenize(text) if w.isalpha()]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2WLv3CLTH52"
      },
      "source": [
        "Get vocabulary from corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoD3Ef2ETHfO"
      },
      "source": [
        "vocab = {}\n",
        "for token in tokenized_text:\n",
        "  vocab[token] = vocab.get(token, 0) + 1"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgauWw7kAsim"
      },
      "source": [
        "Limit vocabulary to the $n$ most common words and assign unique number to each token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ind8h_4QA8D3"
      },
      "source": [
        "def word_to_index(tokens):\n",
        "\n",
        "  word_to_id = {k:i for i,(k,v) in enumerate(sorted(tokens.items(), key=lambda item: item[1], reverse=True))}\n",
        "  return word_to_id\n",
        "\n",
        "word_to_id = word_to_index(vocab)\n",
        "total_vocab = list(word_to_id.keys())"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1XYDDViAWGb"
      },
      "source": [
        "def index_to_word(dic_inds, ind):\n",
        "  token = list(dic_inds.keys())[list(dic_inds.values()).index(ind)] \n",
        "\n",
        "  return token"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpjzYYBwRW8j"
      },
      "source": [
        "Create sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx2EvZH7MZXK"
      },
      "source": [
        "def create_seq(text, seq_len = 5):\n",
        "    \n",
        "    sequences = []\n",
        "\n",
        "    if len(text) > seq_len:\n",
        "      for i in range(seq_len, len(text)):\n",
        "        seq = text[i-seq_len:i+1]\n",
        "        sequences.append(\" \".join(seq))\n",
        "\n",
        "      return sequences\n",
        "\n",
        "    else:\n",
        "      \n",
        "      return [text]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whkZwoM5MaMw"
      },
      "source": [
        "sequences = create_seq(tokenized_text)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yITxygXTRY2t"
      },
      "source": [
        "Split sequences into inputs and targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIYgZ72ONhBr"
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for seq in sequences:\n",
        "  X.append(\" \".join(seq.split()[:-1]))\n",
        "  y.append(\" \".join(seq.split()[1:]))"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_E8nWG2Rdn5"
      },
      "source": [
        "Convert inputs and targets to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hktBQuWgOEVO"
      },
      "source": [
        "def to_integer_sequence(sequence):\n",
        "  return [word_to_id[token] for token in word_tokenize(sequence) if word_to_id.get(token,0) != 0]\n",
        "\n",
        "X_int = [to_integer_sequence(seq) for seq in X if len(to_integer_sequence(seq)) >= 5]\n",
        "y_int = [to_integer_sequence(seq) for seq in y if len(to_integer_sequence(seq)) >= 5]\n",
        "\n",
        "X_int = np.array(X_int, dtype=int)\n",
        "y_int = np.array(y_int, dtype=int)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfCplxFsQWGi"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sisVXimkEuhK"
      },
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzUHpzQrM3QE"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xPR395mmFrK"
      },
      "source": [
        "ts = 0.25\n",
        "n = 1000\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_int[:-1], y_int, test_size=ts,\n",
        "                                                  random_state=1)\n",
        "\n",
        "features_train = torch.from_numpy(X_train).type(torch.long)\n",
        "targets_train = torch.from_numpy(y_train).type(torch.long) \n",
        "\n",
        "features_val = torch.from_numpy(X_val).type(torch.long)\n",
        "targets_val = torch.from_numpy(y_val).type(torch.long) \n",
        "\n",
        "train_data = data_utils.TensorDataset(features_train, targets_train)\n",
        "val_data = data_utils.TensorDataset(features_val, targets_val)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmHT76mc4-TL"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_size, n_hidden, n_layers,\n",
        "                 drop_prob):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        \n",
        "        self.emb_layer = nn.Embedding(vocab_size, embedding_size)\n",
        "      \n",
        "        self.lstm = nn.LSTM(embedding_size, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        embeddings = self.emb_layer(x)     \n",
        "        \n",
        "        lstm_output, hidden = self.lstm(embeddings, hidden)\n",
        "        \n",
        "        out = self.dropout(lstm_output)\n",
        "        \n",
        "        out = out.reshape(-1, self.n_hidden) \n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(self.device),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(self.device))\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtDhpMCxsTc6"
      },
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train(model, train_data, val_data, optimizer, criterion, bs, epochs, clip):\n",
        "    \n",
        "    # Model to device: cuda (if available) or cpu\n",
        "    model.to(device)\n",
        "\n",
        "    # Training data\n",
        "    train_loader = data_utils.DataLoader(train_data, batch_size=bs,\n",
        "                                          shuffle=True, drop_last=True)\n",
        "    # Validation data\n",
        "    validation_loader = data_utils.DataLoader(val_data, batch_size=bs,\n",
        "                                          shuffle=True, drop_last=True)\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for e in range(epochs):\n",
        "\n",
        "        # initialize hidden state\n",
        "        h = model.init_hidden(bs)\n",
        "        train_loss_epoch = 0\n",
        "        for inputs_train, targets_train in train_loader:\n",
        "            \n",
        "            # Tensors to device: cuda (if available) or cpu\n",
        "            inputs_train, targets_train = inputs_train.to(device), targets_train.to(device)\n",
        "\n",
        "            # Detach hidden states\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # Remove accumulated gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get output and hidden layer from forward pass\n",
        "            output, h = model(inputs_train, h)\n",
        "\n",
        "            # Get loss \n",
        "            loss = criterion(output, targets_train.view(-1))\n",
        "            train_loss_epoch += loss.item()\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip to prevent exploding gradient \n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "            # Update weigths\n",
        "            optimizer.step()\n",
        "\n",
        "        train_losses.append(train_loss_epoch / train_loader.batch_size) \n",
        "\n",
        "        valid_loss_epoch = 0\n",
        "        h = model.init_hidden(bs)\n",
        "        model.eval()\n",
        "        with torch.no_grad():  \n",
        "          for inputs_val, targets_val in validation_loader:\n",
        "              \n",
        "              inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "\n",
        "              h = tuple([each.data for each in h])\n",
        "              # Forward Pass\n",
        "              output, h = model(inputs_val, h)\n",
        "              # Get Loss\n",
        "              loss = criterion(output, targets_val.view(-1))\n",
        "              valid_loss_epoch += loss.item()\n",
        "\n",
        "          val_losses.append(valid_loss_epoch / validation_loader.batch_size)   \n",
        "\n",
        "        clear_output(wait=True)                          \n",
        "        plt.plot(train_losses, label='Training')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.plot(val_losses, label='Validation')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend(loc='best')\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
        "          \"| Training Loss: {}\".format(train_loss_epoch),\n",
        "          \"| Validation Loss: {}\".format(valid_loss_epoch))\n",
        "\n",
        "    return model\n",
        "\n",
        "def predict(model, tkn, h=None):      \n",
        "  # tensor inputs\n",
        "  word_id = np.array([[word_to_id[tkn]]])\n",
        "  input_ids = torch.from_numpy(word_id)\n",
        "  \n",
        "  # Tensors to device: cuda (if available) or cpu\n",
        "  inputs = input_ids.to(device)\n",
        "\n",
        "  # Detach hidden state from history\n",
        "  h = tuple([each.data for each in h])\n",
        "\n",
        "  # Get output and hidden layer\n",
        "  out, h = model(inputs, h)\n",
        "\n",
        "  # Get probabilities\n",
        "  probs = F.softmax(out, dim=1).data\n",
        "\n",
        "  probs = probs.cpu().numpy()\n",
        "\n",
        "  probs = probs.reshape(probs.shape[1],)\n",
        "\n",
        "  # Indices for top n values\n",
        "  n = 5\n",
        "  top_n_idx = probs.argsort()[-n:][::-1]\n",
        "\n",
        "  # Sample from the top 5\n",
        "  inds = list(range(n))\n",
        "  sampled_token_index = top_n_idx[random.sample(inds,1)[0]]\n",
        "\n",
        "  # Decode inds into its corresponding word\n",
        "  token = index_to_word(word_to_id, sampled_token_index)\n",
        "  return token, h\n",
        "\n",
        "\n",
        "# function to generate text\n",
        "def sample(model, size, prime='She'):\n",
        "    # Model to device (cuda or cpu)\n",
        "    model.to(device)\n",
        "    # Evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Batch size of 1\n",
        "    h = model.init_hidden(1)\n",
        "    # Split input tokens\n",
        "    toks = prime.split()\n",
        "\n",
        "    # Predict next token using trained model\n",
        "    for t in prime.split():\n",
        "      token, h = predict(model, t, h)\n",
        "    \n",
        "    toks.append(token)\n",
        "\n",
        "    # Predict next tokens\n",
        "    for i in range(size-1):\n",
        "        token, h = predict(model, toks[-1], h)\n",
        "        toks.append(token)\n",
        "\n",
        "    return ' '.join(toks)\n",
        "            "
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjA7l47RML49"
      },
      "source": [
        "e_s = 100\n",
        "n_h = 512\n",
        "n_l = 2\n",
        "d_o = 0.2\n",
        "model = LSTM(len(total_vocab), embedding_size=e_s, n_hidden=n_h, n_layers=n_l,\n",
        "             drop_prob=d_o)\n",
        "\n",
        "bs = 64\n",
        "e = 10\n",
        "lr = 1e-4\n",
        "c = 1\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "trained_model = train(model, train_data, val_data, optimizer, criterion, bs, e,\n",
        "                      clip=c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB1Ufmp8MM_g"
      },
      "source": [
        "first_tokens = 'She has'\n",
        "predicted_length = 10\n",
        "sample(trained_model, predicted_length, first_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}